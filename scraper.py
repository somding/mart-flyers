import asyncio
import json
import os
import aiohttp
import hashlib
import re
import time
from playwright.async_api import async_playwright
from PIL import Image

# ==========================================
# âš™ï¸ ì„¤ì • (Configuration)
# ==========================================
DATA_FILE = 'data.json'
IMAGES_DIR = 'images'
USER_AGENT = 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1'

# ë§ˆíŠ¸ë³„ ì „ë‹¨ì§€ URL
EMART_URL = 'https://eapp.emart.com/leaflet/leafletView_EL.do'
HOMEPLUS_URL = 'https://my.homeplus.co.kr/leaflet'
LOTTE_URL = 'https://www.mlotte.net/leaflet?rst1=HYPER'

# ==========================================
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (Utility Functions)
# ==========================================

def calculate_file_hash(filepath):
    """
    (Deprecated) íŒŒì¼ì˜ MD5 í•´ì‹œê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, ì¶”í›„ ì—„ê²©í•œ ë¹„êµê°€ í•„ìš”í•  ë•Œë¥¼ ëŒ€ë¹„í•´ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
    """
    if not os.path.exists(filepath):
        return None
    try:
        # ì´ë¯¸ì§€ í”½ì…€ ë°ì´í„°ë§Œ í•´ì‹±í•˜ì—¬ ë©”íƒ€ë°ì´í„° ë³€ê²½ ë¬´ì‹œ
        with Image.open(filepath) as img:
            pixel_data = img.tobytes()
            hash_md5 = hashlib.md5()
            hash_md5.update(pixel_data)
            return hash_md5.hexdigest()
    except Exception:
        pass
        
    try:
        hash_md5 = hashlib.md5()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception:
        return None

def is_image_different(path1, path2):
    """
    ë‘ ì´ë¯¸ì§€ íŒŒì¼ì´ 'ì‹¤ì§ˆì ìœ¼ë¡œ' ë‹¤ë¥¸ì§€ ë¹„êµí•©ë‹ˆë‹¤.
    ì„œë²„ì˜ ì¬ì••ì¶•ìœ¼ë¡œ ì¸í•œ ë¯¸ì„¸í•œ íŒŒì¼ í¬ê¸° ì°¨ì´ë¥¼ ë¬´ì‹œí•˜ê¸° ìœ„í•´
    íŒŒì¼ í¬ê¸° ì˜¤ì°¨ê°€ 3% ë¯¸ë§Œì´ê³  í•´ìƒë„ê°€ ê°™ìœ¼ë©´ 'ê°™ì€ íŒŒì¼'ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
    
    Args:
        path1 (str): ì²« ë²ˆì§¸ íŒŒì¼ ê²½ë¡œ
        path2 (str): ë‘ ë²ˆì§¸ íŒŒì¼ ê²½ë¡œ
    Returns:
        bool: ë‹¤ë¥´ë©´ True, ë¹„ìŠ·í•˜ê±°ë‚˜ ê°™ìœ¼ë©´ False
    """
    if not os.path.exists(path1) or not os.path.exists(path2):
        return True # íŒŒì¼ì´ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ 'ë‹¤ë¦„' (ë³€ê²½ë¨)
    
    try:
        # 1. íŒŒì¼ í¬ê¸° ë¹„êµ
        size1 = os.path.getsize(path1)
        size2 = os.path.getsize(path2)
        
        # 0ë°”ì´íŠ¸ íŒŒì¼ì€ ë¬´íš¨í•¨
        if size1 == 0 or size2 == 0: return True
        
        # í¬ê¸° ì°¨ì´ ë¹„ìœ¨ ê³„ì‚°
        diff_ratio = abs(size1 - size2) / max(size1, size2)
        
        # 2. ìœ ì‚¬ë„ íŒë‹¨ (3% ë¯¸ë§Œ ì°¨ì´ & í•´ìƒë„ ì¼ì¹˜)
        if diff_ratio < 0.03:
            with Image.open(path1) as img1, Image.open(path2) as img2:
                if img1.size == img2.size:
                    return False # í¬ê¸°ë„ ë¹„ìŠ·í•˜ê³  í•´ìƒë„ë„ ê°™ìŒ -> ë³€ê²½ ì—†ìŒ
        
        return True # ì°¨ì´ê°€ í¬ë¯€ë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ì„
    except Exception:
        # íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨ ë“± ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ 'ë‹¤ë¦„'ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì—…ë°ì´íŠ¸ ìœ ë„
        return True

async def download_image(session, url, filename):
    """
    ì´ë¯¸ì§€ë¥¼ ë¹„ë™ê¸°ë¡œ ë‹¤ìš´ë¡œë“œí•˜ê³  ìœ íš¨ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.
    ë„ˆë¬´ ì‘ê±°ë‚˜(ì•„ì´ì½˜), ì˜ëª»ëœ í˜•ì‹ì˜ ì´ë¯¸ì§€ëŠ” ì €ì¥í•˜ì§€ ì•Šê³  ì‚­ì œí•©ë‹ˆë‹¤.
    """
    if not url: return None
    
    try:
        # ìŠ¤í‚¤ë§ˆ ë³´ì • (//example.com -> https://example.com)
        if url.startswith('//'):
            url = 'https:' + url
        elif url.startswith('/') and not url.startswith('http'):
            # ë„ë©”ì¸ì„ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì£¼ì˜ í•„ìš”, í˜¸ì¶œë¶€ì—ì„œ full url ê¶Œì¥
            pass 

        async with session.get(url) as response:
            if response.status == 200:
                filepath = os.path.join(IMAGES_DIR, filename)
                content = await response.read()
                
                # íŒŒì¼ ì“°ê¸°
                with open(filepath, 'wb') as f:
                    f.write(content)
                
                # --- í’ˆì§ˆ ê²€ì‚¬ (Validation) ---
                file_size = len(content)
                
                # 1. í¬ê¸° í•„í„° (1KB ë¯¸ë§Œ ì‚­ì œ)
                if file_size < 1000:
                    os.remove(filepath)
                    return None
                
                # 2. ì´ë¯¸ì§€ í¬ë§· í•„í„° (JPG/PNG í—¤ë” í™•ì¸)
                if not (content.startswith(b'\xff\xd8\xff') or content.startswith(b'\x89PNG')):
                    os.remove(filepath)
                    return None
                
                # 3. í•´ìƒë„ í•„í„° (PIL ì‚¬ìš©)
                try:
                    with Image.open(filepath) as img:
                        w, h = img.size
                        # ê°€ë¡œ/ì„¸ë¡œ 300px ë¯¸ë§Œì´ë©´ ì•„ì´ì½˜ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ì‚­ì œ
                        if w < 300 or h < 300:
                            os.remove(filepath)
                            return None
                        
                        # ë¹„ìœ¨ í•„í„°: ê°€ë¡œê°€ ì„¸ë¡œë³´ë‹¤ ë„ˆë¬´ ê¸¸ë©´(ë°°ë„ˆ ë“±) ì‚­ì œ
                        if w > h * 3.0: 
                            os.remove(filepath)
                            return None
                except Exception:
                    os.remove(filepath)
                    return None
                    
                return f"./{IMAGES_DIR}/{filename}"
            else:
                 return None
    except Exception as e:
        print(f"[-] ë‹¤ìš´ë¡œë“œ ì—ëŸ¬ ({url}): {e}")
        return None

# ==========================================
# ğŸ›’ ë§ˆíŠ¸ë³„ í¬ë¡¤ë§ í•¨ìˆ˜ (Scraping)
# ==========================================

async def scrape_emart(context, session):
    """
    [ì´ë§ˆíŠ¸] ìˆœì°¨ì  í˜ì´ì§€ ë°©ë¬¸ ë°©ì‹
    - 'ë‹¤ìŒ' ë²„íŠ¼ì„ í´ë¦­í•˜ë©° í˜ì´ì§€ë¥¼ ë„˜ê¸°ê³ , ì¤‘ì•™ì— ìˆëŠ” í° ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    print(f"[ì´ë§ˆíŠ¸] í¬ë¡¤ë§ ì‹œì‘...")
    page = await context.new_page()
    images = []
    
    try:
        await page.goto(EMART_URL, timeout=60000)
        await page.wait_for_load_state('networkidle')
        await page.wait_for_timeout(2000)
        
        print("[ì´ë§ˆíŠ¸] í˜ì´ì§€ ìˆœíšŒ ì¤‘...")
        for i in range(20): # ì•ˆì „ì„ ìœ„í•´ ìµœëŒ€ 20í˜ì´ì§€ ì œí•œ
            try:
                # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê°€ì¥ ìœ ë ¥í•œ ì „ë‹¨ì§€ ì´ë¯¸ì§€ ì¶”ì¶œ
                visible_img_src = await page.evaluate('''() => {
                    const imgs = Array.from(document.querySelectorAll('img'));
                    // 300px ì´ìƒì´ê³  ë¡œê³ ê°€ ì•„ë‹Œ ì´ë¯¸ì§€ í•„í„°ë§
                    const candidates = imgs.filter(img => {
                        const rect = img.getBoundingClientRect();
                        return rect.width > 300 && rect.height > 300 && 
                               !img.src.includes('logo') && !img.src.includes('icon');
                    });
                    return candidates.length > 0 ? candidates[0].src : null;
                }''')

                if visible_img_src:
                    # ì„ì‹œ íŒŒì¼ëª… ìƒì„± (temp_emart_XX.jpg)
                    count = len(images) + 1
                    filename = f"temp_emart_{count:02d}.jpg"
                    
                    # ì¤‘ë³µ URL ì²´í¬
                    if not any(item['url'] == visible_img_src for item in images):
                        print(f"  + {count}í˜ì´ì§€ ì´ë¯¸ì§€ ë°œê²¬")
                        images.append({'url': visible_img_src, 'filename': filename})
            except Exception:
                pass

            # 'ë‹¤ìŒ' ë²„íŠ¼ í´ë¦­
            try:
                btn = await page.query_selector('.btn_next') or await page.query_selector('.d-next')
                if btn and await btn.is_visible():
                    await btn.click()
                    await page.wait_for_timeout(1000)
                else:
                    break # ë²„íŠ¼ ì—†ìœ¼ë©´ ì¢…ë£Œ
            except Exception:
                break
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ë³‘ë ¬)
        print(f"[ì´ë§ˆíŠ¸] ì´ {len(images)}ì¥ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
        tasks = [download_image(session, item['url'], item['filename']) for item in images]
        if tasks:
            results = await asyncio.gather(*tasks)
            # ì„±ê³µí•œ íŒŒì¼ ê²½ë¡œë§Œ ë°˜í™˜
            return [r for r in results if r is not None]

    except Exception as e:
        print(f"[ì´ë§ˆíŠ¸] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        await page.close()
    
    return []

async def scrape_homeplus(context, session):
    """
    [í™ˆí”ŒëŸ¬ìŠ¤] ì¢Œí‘œ ì •ë ¬ ë°©ì‹
    - ì´ë¯¸ì§€ê°€ Lazy Loading ë˜ë¯€ë¡œ ìŠ¤í¬ë¡¤ì„ ëê¹Œì§€ ë‚´ë¦½ë‹ˆë‹¤.
    - DOM ìˆœì„œê°€ ì„ì—¬ìˆìœ¼ë¯€ë¡œ, ì´ë¯¸ì§€ì˜ Yì¢Œí‘œ(Top) ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì˜¬ë°”ë¥¸ ìˆœì„œë¥¼ ë§ì¶¥ë‹ˆë‹¤.
    """
    print(f"[í™ˆí”ŒëŸ¬ìŠ¤] í¬ë¡¤ë§ ì‹œì‘...")
    page = await context.new_page()
    final_images = []
    
    try:
        await page.goto(HOMEPLUS_URL, timeout=60000)
        await page.wait_for_load_state('networkidle')
        
        # ìŠ¤í¬ë¡¤ ìµœí•˜ë‹¨ ì´ë™ (ì´ë¯¸ì§€ ë¡œë”© ìœ ë„)
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await page.wait_for_timeout(2000)
        
        # ì´ë¯¸ì§€ ì •ë³´(src, ì¢Œí‘œ, í¬ê¸°) ì¶”ì¶œ
        img_data = await page.evaluate('''() => {
            const imgs = Array.from(document.querySelectorAll('img'));
            return imgs.map(img => {
                const rect = img.getBoundingClientRect();
                return {
                    src: img.src,
                    top: rect.top + window.scrollY,
                    width: rect.width,
                    height: rect.height
                };
            }).filter(item => {
                // í¬ê¸° > 200px, ë¡œê³  ì œì™¸, leaflet/flyer/jpg í‚¤ì›Œë“œ í¬í•¨
                return item.width > 200 && 
                       item.height > 200 &&
                       !item.src.includes('logo') &&
                       (item.src.includes('leaflet') || item.src.includes('flyer') || item.src.includes('jpg'));
            });
        }''')
        
        # Yì¢Œí‘œ ê¸°ì¤€ ì •ë ¬ ë° ì¤‘ë³µ ì œê±°
        sorted_img_data = sorted(img_data, key=lambda x: x['top'])
        unique_urls = []
        seen = set()
        
        for item in sorted_img_data:
            src = item['src']
            if not src.startswith('http'): # ìƒëŒ€ê²½ë¡œ ë³´ì •
                 src = 'https://my.homeplus.co.kr' + src if src.startswith('/') else src
            
            if src not in seen:
                seen.add(src)
                unique_urls.append(src)
                if len(unique_urls) >= 15: break # ìµœëŒ€ 15ì¥
        
        # ë‹¤ìš´ë¡œë“œ (ìˆœì„œ ìœ ì§€: temp íŒŒì¼ ë²ˆí˜¸ ë¶€ì—¬)
        print(f"[í™ˆí”ŒëŸ¬ìŠ¤] {len(unique_urls)}ì¥ ë‹¤ìš´ë¡œë“œ ì‹œë„...")
        tasks = []
        for idx, src in enumerate(unique_urls):
             filename = f"temp_homeplus_{idx+1:02d}.jpg"
             tasks.append(download_image(session, src, filename))
        
        if tasks:
            results = await asyncio.gather(*tasks)
            # ê²°ê³¼ ì •ë ¬ (íŒŒì¼ëª… ìˆœ)
            valid_results = [r for r in results if r is not None]
            final_images = sorted(valid_results)

    except Exception as e:
        print(f"[í™ˆí”ŒëŸ¬ìŠ¤] ì˜¤ë¥˜: {e}")
    finally:
        await page.close()
    
    return final_images

async def scrape_lotte(context, session):
    """
    [ë¡¯ë°ë§ˆíŠ¸] URL íŒŒë¼ë¯¸í„° ë°©ì‹
    - ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„°(?rst1=HYPER)ë¡œ ì ‘ì†í•´ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    print(f"[ë¡¯ë°ë§ˆíŠ¸] í¬ë¡¤ë§ ì‹œì‘...")
    page = await context.new_page()
    images = []
    
    try:
        await page.goto(LOTTE_URL, timeout=60000)
        await page.wait_for_load_state('networkidle')
        await page.wait_for_timeout(3000)
        
        # ë¡¯ë°ë§ˆíŠ¸ëŠ” body ìŠ¤í¬ë¡¤ì´ ì•„ë‹ ìˆ˜ ìˆìŒ, ê·¸ë˜ë„ ì‹œë„
        try:
             await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        except: pass
        
        img_elements = await page.query_selector_all('img')
        
        tasks = []
        seen_urls = set()
        count = 1
        
        for img in img_elements:
            src = await img.get_attribute('src')
            data_src = await img.get_attribute('data-src')
            real_src = data_src or src
            
            if real_src:
                 # ìƒëŒ€ê²½ë¡œ -> ì ˆëŒ€ê²½ë¡œ
                 if real_src.startswith('//'):
                     real_src = 'https:' + real_src
                 elif real_src.startswith('/'):
                     real_src = 'https://www.mlotte.net' + real_src
                 
                 # í•„í„°ë§
                 if 'logo' in real_src or 'icon' in real_src: continue
                 if real_src in seen_urls: continue
                 
                 if 'jpg' in real_src or 'png' in real_src:
                    seen_urls.add(real_src)
                    filename = f"temp_lotte_{count:02d}.jpg"
                    tasks.append(download_image(session, real_src, filename))
                    count += 1
                    if count > 20: break
        
        if tasks:
            results = await asyncio.gather(*tasks)
            images = sorted([r for r in results if r is not None])

    except Exception as e:
        print(f"[ë¡¯ë°ë§ˆíŠ¸] ì˜¤ë¥˜: {e}")
    finally:
        await page.close()
    
    return images

# ==========================================
# ğŸš€ ë©”ì¸ ë° ì—…ë°ì´íŠ¸ ë¡œì§ (Main Workflow)
# ==========================================

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        # ëª¨ë°”ì¼ í™˜ê²½ ì—ë®¬ë ˆì´ì…˜
        context = await browser.new_context(
            viewport={'width': 390, 'height': 844},
            user_agent=USER_AGENT,
            locale='ko-KR'
        )
        
        async with aiohttp.ClientSession() as session:
            # 1. ë°ì´í„° ë¡œë“œ (íŒŒì¼ ì—†ìœ¼ë©´ ê¸°ë³¸ êµ¬ì¡° ìƒì„±)
            try:
                with open(DATA_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except (FileNotFoundError, json.JSONDecodeError):
                print(f"[Warning] {DATA_FILE} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë§ˆíŠ¸ ì„¤ì •ì´ ì—†ì–´ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                data = [] # ì´ ê²½ìš°ì—” ì‚¬ì‹¤ìƒ ì‹¤íŒ¨, ë³µêµ¬ ë¡œì§ì€ ìƒëµ.

            # 2. í¬ë¡¤ë§ ì‹¤í–‰ (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬)
            print(">>> ì „ì²´ ë§ˆíŠ¸ í¬ë¡¤ë§ ì‹œì‘...")
            # ìˆœì„œ: ì´ë§ˆíŠ¸(0), í™ˆí”ŒëŸ¬ìŠ¤(1), ë¡¯ë°ë§ˆíŠ¸(2)
            results = await asyncio.gather(
                scrape_emart(context, session),
                scrape_homeplus(context, session),
                scrape_lotte(context, session)
            )
            
            # 3. ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ì•„ì¹´ì´ë¹™ ë¡œì§
            #    new_images: ìƒˆë¡œ ë‹¤ìš´ë¡œë“œëœ ì„ì‹œ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
            def update_mart_data(mart_index, new_images):
                mart_name = data[mart_index]['name']
                
                # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ë“±ìœ¼ë¡œ ìƒˆ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
                if not new_images:
                    print(f"[{mart_name}] ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì—…ë°ì´íŠ¸ ì¤‘ë‹¨.")
                    return

                # íŒŒì¼ëª… ì ‘ë‘ì‚¬ ê²°ì • (ì €ì¥ë  ì´ë¦„)
                if mart_name.startswith('ì´ë§ˆíŠ¸'):   prefix = 'emart'
                elif mart_name.startswith('í™ˆí”ŒëŸ¬ìŠ¤'): prefix = 'homeplus'
                else:                               prefix = 'lotte'
                
                # ë¹„êµ ëŒ€ìƒ: ì§€ê¸ˆ ë‹¤ìš´ë°›ì€ temp íŒŒì¼ë“¤
                temp_files = [p.replace('./', '') for p in new_images]
                
                # ë¹„êµ ì›ë³¸: í˜„ì¬ ì‚´ì•„ìˆëŠ”(Current) íŒŒì¼ë“¤
                current_flyer_info = data[mart_index]['flyers']['current']
                current_files = [p.split('?')[0].replace('./', '') for p in current_flyer_info.get('images', [])]

                # --- ë³€ê²½ ê°ì§€ ë¡œì§ ---
                is_modified = False
                
                # 1) ì¥ìˆ˜ê°€ ë‹¤ë¥´ë©´ ë¬´ì¡°ê±´ ë³€ê²½
                if len(temp_files) != len(current_files):
                    is_modified = True
                    print(f"[{mart_name}] ì—…ë°ì´íŠ¸ ê°ì§€: í˜ì´ì§€ ìˆ˜ ë³€ê²½ ({len(current_files)} -> {len(temp_files)})")
                else:
                    # 2) ì¥ìˆ˜ê°€ ê°™ìœ¼ë©´ ê° ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ íŒŒì¼ í¬ê¸°/í•´ìƒë„ ë“±ìœ¼ë¡œ ë¹„êµ
                    for t_path, c_path in zip(temp_files, current_files):
                        if is_image_different(t_path, c_path):
                            is_modified = True
                            print(f"[{mart_name}] ì—…ë°ì´íŠ¸ ê°ì§€: ì´ë¯¸ì§€ ë‚´ìš© ë³€ê²½ë¨")
                            break
                
                # ë³€ê²½ ì‚¬í•­ì´ ì—†ìœ¼ë©´ ì„ì‹œ íŒŒì¼ ì‚­ì œ í›„ ì¢…ë£Œ
                if not is_modified:
                    print(f"[{mart_name}] ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤. (ë³€ê²½ ì—†ìŒ)")
                    for p in temp_files:
                        if os.path.exists(p): os.remove(p)
                    return

                # --- ì—…ë°ì´íŠ¸ ì‹¤í–‰ (Archive & Promote) ---
                print(f"[{mart_name}] ì—…ë°ì´íŠ¸ë¥¼ ì ìš©í•©ë‹ˆë‹¤...")
                
                # 1. ì•„ì¹´ì´ë¹™: í˜„ì¬(Current) íŒŒì¼ì„ ê³¼ê±°(Past)ë¡œ ì´ë™
                #    íŒŒì¼ëª…ì„ '_new_' -> '_' ë¡œ ë³€ê²½í•˜ê±°ë‚˜ '_past' ì¶”ê°€
                archived_files = []
                if current_files:
                    for old_path in current_files:
                        if not os.path.exists(old_path): continue
                        
                        # íŒŒì¼ëª… ë³€í™˜ ê·œì¹™
                        if '_new_' in old_path:
                            new_path = old_path.replace('_new_', '_')
                        else:
                            new_path = old_path.replace('.jpg', '_past.jpg')
                        
                        # ë®ì–´ì“°ê¸° í—ˆìš© (ê³¼ê±° íŒŒì¼ ê°±ì‹ )
                        if os.path.exists(new_path):
                            os.remove(new_path)
                            
                        try:
                            os.rename(old_path, new_path)
                            archived_files.append(f"./{new_path}")
                        except Exception as e:
                            print(f"  [Warning] ì•„ì¹´ì´ë¹™ íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {e}")

                    # JSON ë°ì´í„° ê°±ì‹  (Past)
                    if archived_files:
                         data[mart_index]['flyers']['past']['images'] = archived_files
                         print(f"  -> ì§€ë‚œ ì „ë‹¨ì§€ë¡œ ì´ë™ë¨ ({len(archived_files)}ì¥)")

                # 2. ìµœì‹ í™”: ì„ì‹œ(Temp) íŒŒì¼ì„ í˜„ì¬(Current)ë¡œ ìŠ¹ê²©
                #    temp_prefix_XX.jpg -> prefix_new_XX.jpg
                final_current_files = []
                for idx, temp_path in enumerate(temp_files):
                    if not os.path.exists(temp_path): continue
                    
                    final_name = f"{IMAGES_DIR}/{prefix}_new_{idx+1:02d}.jpg"
                    
                    if os.path.exists(final_name):
                        os.remove(final_name)
                    
                    try:
                        os.rename(temp_path, final_name)
                        final_current_files.append(f"./{final_name}")
                    except Exception as e:
                         print(f"  [Error] ìµœì‹  íŒŒì¼ ì ìš© ì‹¤íŒ¨: {e}")
                
                # JSON ë°ì´í„° ê°±ì‹  (Current)
                data[mart_index]['flyers']['current']['images'] = final_current_files
                data[mart_index]['flyers']['current']['date'] = time.strftime("%Y-%m-%d") # ë‚ ì§œ ê°±ì‹ 
                print(f"  -> ìµœì‹  ì „ë‹¨ì§€ ì ìš© ì™„ë£Œ ({len(final_current_files)}ì¥)")

            # ê° ë§ˆíŠ¸ ë°ì´í„° ê°±ì‹  ì‹¤í–‰
            update_mart_data(0, results[0]) # ì´ë§ˆíŠ¸
            update_mart_data(1, results[1]) # í™ˆí”ŒëŸ¬ìŠ¤
            update_mart_data(2, results[2]) # ë¡¯ë°ë§ˆíŠ¸

            # 4. ìµœì¢… ê²°ê³¼ ì €ì¥
            with open(DATA_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            
            print(">>> ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ê°€ ì•ˆì „í•˜ê²Œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        await browser.close()

if __name__ == '__main__':
    start_time = time.time()
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n[!] ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[Error] ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        print(f"--- ì´ ì‹¤í–‰ ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ ---")
